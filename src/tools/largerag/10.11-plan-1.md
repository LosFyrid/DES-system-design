# LargeRAG 工具实现方案（v1.0）

> **文档性质**: 技术实施方案
> **制定日期**: 2025-10-11
> **目标读者**: 开发实习生
> **方案状态**: 待实施

---

## 1. 项目背景与目标

### 1.1 项目定位
LargeRAG 是 DES 配方预测系统的三大工具之一，负责**大规模文献检索**（目标：~10,000 篇论文），为 Reasoning Agent 提供背景知识和补充信息。

### 1.2 核心职责
- 索引和存储约 10,000 篇 DES 相关文献
- 根据查询请求检索相关文献片段
- 返回格式化的检索结果供 Reasoning Agent 使用

### 1.3 与其他工具的协作关系
```
Reasoning Agent（调度器）
    ├─ CoreRAG: 少量核心文献，本体驱动，理论原理
    ├─ LargeRAG: 10K 文献，向量检索，背景知识  ← 本方案
    └─ 实验数据工具: 数值数据查询（设计中）
```

---

## 2. 技术选型（强制约束）

### 2.1 必须使用的技术栈

| 组件类型 | 指定技术 | 版本要求 | 原因 |
|---------|---------|---------|------|
| **主框架** | LlamaIndex | >= 0.10.0 | 高层抽象，快速开发 |
| **Embedding 模型** | Qwen (DashScope API) | text-embedding-v2/v3 | 项目统一 Qwen 系列 |
| **Reranker 模型** | Qwen (DashScope API) | gte-rerank | 提升检索准确性 |
| **LLM 模型** | Qwen3-235B (DashScope API) | qwen-max | 生成式回答 |
| **向量数据库** | Chroma | >= 0.4.0 | 轻量、易部署、持久化 |
| **缓存系统** | Redis | >= 7.0 | 避免重复计算 embedding |
| **配置管理** | YAML + Python | 3.13+ | 复用 CoreRAG 模式 |

### 2.2 禁止使用的技术
❌ **禁止**使用 FAISS（难以持久化）
❌ **禁止**使用 Pinecone/Weaviate（引入云服务依赖）
❌ **禁止**使用 OpenAI Embeddings（必须用 Qwen）
❌ **禁止**手动实现向量检索逻辑（必须使用 LlamaIndex）

---

## 3. 系统架构设计（强制规范）

### 3.1 目录结构（严格执行）

```
src/tools/largerag/
├── __init__.py                    # 包初始化，导出 LargeRAG 主类
├── README.md                      # 使用文档
├── requirements.txt               # 依赖清单
│
├── config/                        # 配置模块
│   ├── __init__.py
│   ├── settings.yaml              # YAML 配置文件
│   └── settings.py                # 配置加载器（复用 CoreRAG 模式）
│
├── core/                          # 核心功能模块
│   ├── __init__.py
│   ├── document_processor.py     # JSON → LlamaIndex Document 转换
│   ├── indexer.py                # 索引构建和管理
│   └── query_engine.py           # 查询引擎封装
│
├── largerag.py                    # 主接口类（对外统一 API）
│
└── tests/                         # 测试模块
    ├── __init__.py
    ├── test_document_processor.py
    ├── test_indexer.py
    └── test_integration.py        # 端到端测试（100 篇小规模数据）
```

**约束规则**:
1. **禁止**在根目录创建除上述之外的任何文件或目录
2. 所有业务逻辑**必须**放在 `core/` 模块
3. 配置文件**禁止**硬编码路径，必须使用 `${PROJECT_ROOT}` 变量

### 3.2 配置文件规范

#### 3.2.1 `config/settings.yaml`（完整模板）

```yaml
# LargeRAG 配置文件
# 注意：敏感信息（API Key）从 .env 读取，不写在此文件

# ============ Embedding 配置 ============
embedding:
  provider: "dashscope"                          # 固定值，不可修改
  model: "text-embedding-v3"                    # Qwen embedding 模型
  text_type: "document"                         # document 或 query
  batch_size: 25                                # 批处理大小（DashScope 限制）
  dimension: 1024                               # 向量维度

# ============ 向量存储配置 ============
vector_store:
  type: "chroma"                                # 固定值，不可修改
  persist_directory: "${PROJECT_ROOT}data/largerag/chroma_db"
  collection_name: "des_literature_v1"
  distance_metric: "cosine"                     # cosine, l2, ip

# ============ 文档处理配置 ============
document_processing:
  chunk_size: 512                               # 分块大小（token）
  chunk_overlap: 50                             # 重叠大小（token）
  separator: "\n\n"                             # 分块分隔符

# ============ 检索配置 ============
retrieval:
  similarity_top_k: 20                          # 向量检索召回数量
  rerank_top_n: 5                               # Reranker 最终返回数量
  similarity_threshold: 0.7                     # 相似度阈值（过滤低分）

# ============ Reranker 配置 ============
reranker:
  provider: "dashscope"                         # 固定值
  model: "gte-rerank"                           # Qwen reranker 模型
  enabled: true                                 # 是否启用 reranker

# ============ LLM 配置 ============
llm:
  provider: "dashscope"                         # 固定值
  model: "qwen-max"                             # Qwen3-235B
  temperature: 0.1
  max_tokens: 2000

# ============ 缓存配置 ============
cache:
  enabled: true                                 # 是否启用缓存
  type: "redis"                                 # redis 或 local
  redis_host: "localhost"
  redis_port: 6379
  redis_db: 0
  collection_name: "largerag_embedding_cache"
  ttl: 86400                                    # 缓存过期时间（秒）

# ============ 日志配置 ============
logging:
  level: "INFO"                                 # DEBUG, INFO, WARNING, ERROR
  file_path: "${PROJECT_ROOT}logs/largerag.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

**配置约束**:
- `provider` 字段**禁止**修改为 `dashscope` 以外的值
- `PROJECT_ROOT` 环境变量**必须**在运行前设置
- API Key **禁止**写在此文件，必须从 `.env` 读取

#### 3.2.2 `config/settings.py`（加载器实现规范）

```python
"""
配置加载器
要求：
1. 支持 ${PROJECT_ROOT} 变量替换
2. 支持 {{key}} 内部引用
3. 从 .env 读取 DASHSCOPE_API_KEY
4. 提供类型安全的配置对象
"""

import os
import re
import yaml
from pathlib import Path
from dataclasses import dataclass
from typing import Optional
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

# ============ 配置数据类 ============
@dataclass
class EmbeddingConfig:
    provider: str
    model: str
    text_type: str
    batch_size: int
    dimension: int

@dataclass
class VectorStoreConfig:
    type: str
    persist_directory: str
    collection_name: str
    distance_metric: str

@dataclass
class RetrievalConfig:
    similarity_top_k: int
    rerank_top_n: int
    similarity_threshold: float

@dataclass
class RerankerConfig:
    provider: str
    model: str
    enabled: bool

@dataclass
class LLMConfig:
    provider: str
    model: str
    temperature: float
    max_tokens: int

@dataclass
class CacheConfig:
    enabled: bool
    type: str
    redis_host: str
    redis_port: int
    redis_db: int
    collection_name: str
    ttl: int

@dataclass
class LargeRAGSettings:
    embedding: EmbeddingConfig
    vector_store: VectorStoreConfig
    retrieval: RetrievalConfig
    reranker: RerankerConfig
    llm: LLMConfig
    cache: CacheConfig

# ============ YAML 变量替换 ============
def resolve_yaml_variables(yaml_dict: dict) -> dict:
    """
    解析 YAML 中的变量引用
    支持：${ENV_VAR} 和 {{key}}
    """
    # 实现逻辑：参考 CoreRAG 的 settings.py
    # 此处省略具体实现...
    pass

# ============ 配置加载 ============
def load_settings(config_path: Optional[str] = None) -> LargeRAGSettings:
    """加载配置文件"""
    if config_path is None:
        config_path = Path(__file__).parent / "settings.yaml"

    with open(config_path, 'r', encoding='utf-8') as f:
        yaml_data = yaml.safe_load(f)

    # 设置 PROJECT_ROOT
    if 'PROJECT_ROOT' not in os.environ:
        # 自动推断项目根目录（向上查找 .git 目录）
        current = Path(__file__).parent
        while current != current.parent:
            if (current / '.git').exists():
                os.environ['PROJECT_ROOT'] = str(current) + '/'
                break
            current = current.parent

    # 变量替换
    resolved = resolve_yaml_variables(yaml_data)

    # 构建配置对象
    settings = LargeRAGSettings(
        embedding=EmbeddingConfig(**resolved['embedding']),
        vector_store=VectorStoreConfig(**resolved['vector_store']),
        retrieval=RetrievalConfig(**resolved['retrieval']),
        reranker=RerankerConfig(**resolved['reranker']),
        llm=LLMConfig(**resolved['llm']),
        cache=CacheConfig(**resolved['cache']),
    )

    return settings

# ============ API Key 获取 ============
def get_dashscope_api_key() -> str:
    """从环境变量获取 DashScope API Key"""
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        raise ValueError(
            "DASHSCOPE_API_KEY not found in environment variables. "
            "Please add it to .env file."
        )
    return api_key

# ============ 全局配置实例 ============
SETTINGS = load_settings()
DASHSCOPE_API_KEY = get_dashscope_api_key()
```

**实现约束**:
1. **必须**使用 `dataclass` 定义配置结构（类型安全）
2. **禁止**在代码中硬编码配置值
3. **必须**提供清晰的错误信息（API Key 缺失时）

---

## 4. 核心模块实现规范

### 4.1 `core/document_processor.py`（文档处理器）

**功能**: 从文献文件夹结构中加载数据并转换为 LlamaIndex Document 对象

**实际数据结构**（基于 35 篇测试文献）:

文献存储在 `data/literature/` 目录下，每篇文献一个文件夹（MD5 哈希命名）：
```
data/literature/
├── {hash1}/
│   ├── article.json              # 段落级结构化数据（详细元数据）
│   ├── content_list_process.json # 处理后的内容列表（用于索引）
│   ├── {hash1}.md                # Markdown 全文（可选）
│   └── images/                   # 图片文件夹
└── {hash2}/
    └── ...
```

**`content_list_process.json` 格式示例**:
```json
[
    {
        "type": "text",
        "text": "Dissolution of biological samples in deep eutectic solvents...",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Deep eutectic solvents (DES) are a new class of green solvents...",
        "page_idx": 0,
        "cites": [
            {"text": "[1]", "ref_id": "BIBREF0"}
        ]
    },
    {
        "type": "image",
        "img_path": "images/xxx.jpg",
        "img_caption": ["Fig. 1. Effect of..."],
        "page_idx": 3
    },
    {
        "type": "table",
        "table_caption": ["Table 1. Analytical performance..."],
        "table_body": "<html>...</html>",
        "page_idx": 5
    }
]
```

**`article.json` 格式示例**（含更多元数据）:
```json
{
    "paragraphs": [
        {
            "paragraph": "Dissolution of biological samples...",
            "type": "body_div",
            "paragraph_idx": 0,
            "pagenum": 0,
            "head": "",
            "text_level": 1
        }
    ]
}
```

**实现规范**:
```python
"""
文档处理器模块
要求：
1. 支持从文件夹结构加载文献数据
2. 优先使用 content_list_process.json（只提取 text 类型）
3. 自动提取元数据（文档哈希、页码、文本层级）
4. 处理缺失字段和异常文件
5. 记录处理日志（跳过的文档、错误）
"""

from typing import List, Dict, Any
from pathlib import Path
import json
import logging
from llama_index.core import Document

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """文献文件夹到 LlamaIndex Document 的转换器"""

    def __init__(self):
        self.processed_count = 0
        self.skipped_count = 0

    def process_from_folders(self, literature_dir: str) -> List[Document]:
        """
        从文献文件夹结构加载数据并转换为 Document 对象

        文件夹结构：
            data/literature/
            ├── {hash1}/
            │   ├── content_list_process.json  # 优先使用（处理后的内容）
            │   ├── article.json               # 备选（段落级数据）
            │   ├── {hash1}.md                 # 不使用（全文 markdown）
            │   └── images/                    # 不处理
            └── {hash2}/
                └── ...

        Args:
            literature_dir: 文献目录路径（如 "data/literature"）

        Returns:
            Document 对象列表（仅包含文本内容）

        Raises:
            FileNotFoundError: 如果目录不存在
        """
        literature_path = Path(literature_dir)
        if not literature_path.exists():
            raise FileNotFoundError(f"Literature directory not found: {literature_dir}")

        documents = []

        # 遍历所有哈希文件夹
        for folder in literature_path.iterdir():
            if not folder.is_dir():
                continue

            doc_hash = folder.name
            content_file = folder / "content_list_process.json"
            article_file = folder / "article.json"

            # 优先使用 content_list_process.json
            if content_file.exists():
                docs = self._load_from_content_list(content_file, doc_hash)
                documents.extend(docs)
            elif article_file.exists():
                logger.warning(f"[{doc_hash}] content_list_process.json not found, using article.json")
                docs = self._load_from_article(article_file, doc_hash)
                documents.extend(docs)
            else:
                logger.error(f"[{doc_hash}] No valid JSON file found, skipping")
                self.skipped_count += 1

        logger.info(f"Total processed: {self.processed_count}, skipped: {self.skipped_count}")
        return documents

    def _load_from_content_list(self, file_path: Path, doc_hash: str) -> List[Document]:
        """
        从 content_list_process.json 加载数据（只提取 text 类型）

        JSON 格式：
        [
            {
                "type": "text",
                "text": "Deep eutectic solvents...",
                "text_level": 1,
                "page_idx": 0,
                "cites": [...]  # 可选
            },
            {
                "type": "image",  # 忽略
                ...
            }
        ]
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content_list = json.load(f)

            documents = []
            for idx, item in enumerate(content_list):
                # 只处理 text 类型
                if item.get("type") != "text":
                    continue

                text = item.get("text", "").strip()
                if not text:
                    logger.warning(f"[{doc_hash}] Item {idx}: empty text, skipping")
                    continue

                # 提取元数据
                metadata = {
                    "doc_hash": doc_hash,
                    "page_idx": item.get("page_idx", -1),
                    "text_level": item.get("text_level", 0),
                    "has_citations": bool(item.get("cites")),
                    "source_file": "content_list_process.json",
                    "item_idx": idx,
                }

                doc = Document(text=text, metadata=metadata)
                documents.append(doc)
                self.processed_count += 1

            logger.info(f"[{doc_hash}] Loaded {len(documents)} text segments from content_list_process.json")
            return documents

        except json.JSONDecodeError as e:
            logger.error(f"[{doc_hash}] Invalid JSON in content_list_process.json: {e}")
            self.skipped_count += 1
            return []
        except Exception as e:
            logger.error(f"[{doc_hash}] Error loading content_list_process.json: {e}")
            self.skipped_count += 1
            return []

    def _load_from_article(self, file_path: Path, doc_hash: str) -> List[Document]:
        """
        从 article.json 加载数据（备选方案）

        JSON 格式：
        {
            "paragraphs": [
                {
                    "paragraph": "Dissolution of...",
                    "type": "body_div",
                    "paragraph_idx": 0,
                    "pagenum": 0,
                    "head": "",
                    "text_level": 1
                }
            ]
        }
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                article_data = json.load(f)

            paragraphs = article_data.get("paragraphs", [])
            documents = []

            for para in paragraphs:
                text = para.get("paragraph", "").strip()
                if not text:
                    continue

                metadata = {
                    "doc_hash": doc_hash,
                    "page_idx": para.get("pagenum", -1),
                    "text_level": para.get("text_level", 0),
                    "paragraph_type": para.get("type", ""),
                    "source_file": "article.json",
                    "paragraph_idx": para.get("paragraph_idx", -1),
                }

                doc = Document(text=text, metadata=metadata)
                documents.append(doc)
                self.processed_count += 1

            logger.info(f"[{doc_hash}] Loaded {len(documents)} paragraphs from article.json")
            return documents

        except json.JSONDecodeError as e:
            logger.error(f"[{doc_hash}] Invalid JSON in article.json: {e}")
            self.skipped_count += 1
            return []
        except Exception as e:
            logger.error(f"[{doc_hash}] Error loading article.json: {e}")
            self.skipped_count += 1
            return []

    def get_statistics(self) -> Dict[str, int]:
        """返回处理统计信息"""
        return {
            "processed": self.processed_count,
            "skipped": self.skipped_count,
            "total": self.processed_count + self.skipped_count
        }
```

**测试要求**:
- 必须测试空文本、缺失字段、无效 JSON 等边界情况
- 必须验证元数据正确提取

### 4.2 `core/indexer.py`（索引构建器）

**功能**: 使用 LlamaIndex + Chroma 构建向量索引

**实现规范**:
```python
"""
索引构建和管理模块
要求：
1. 使用 IngestionPipeline 实现批处理和缓存
2. 支持 Redis 缓存避免重复计算
3. Chroma 持久化存储
4. 提供索引统计信息
"""

from typing import List, Optional
from llama_index.core import VectorStoreIndex, Document
from llama_index.core.ingestion import IngestionPipeline, IngestionCache
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels, DashScopeTextEmbeddingType
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.storage.kvstore.redis import RedisKVStore
import chromadb
import logging

from config.settings import SETTINGS, DASHSCOPE_API_KEY

logger = logging.getLogger(__name__)

class LargeRAGIndexer:
    """向量索引构建和管理器"""

    def __init__(self):
        self.settings = SETTINGS
        self.api_key = DASHSCOPE_API_KEY

        # 初始化 Embedding 模型
        self.embed_model = DashScopeEmbedding(
            model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2,
            text_type=DashScopeTextEmbeddingType.TEXT_TYPE_DOCUMENT,
            api_key=self.api_key,
        )

        # 初始化 Chroma 客户端
        self.chroma_client = chromadb.PersistentClient(
            path=self.settings.vector_store.persist_directory
        )

        # 初始化 Ingestion Pipeline
        self._init_pipeline()

    def _init_pipeline(self):
        """初始化 Ingestion Pipeline（含缓存）"""
        transformations = [
            SentenceSplitter(
                chunk_size=self.settings.document_processing.chunk_size,
                chunk_overlap=self.settings.document_processing.chunk_overlap,
            ),
            self.embed_model,
        ]

        # 配置缓存
        cache = None
        if self.settings.cache.enabled:
            if self.settings.cache.type == "redis":
                cache = IngestionCache(
                    cache=RedisKVStore.from_host_and_port(
                        host=self.settings.cache.redis_host,
                        port=self.settings.cache.redis_port,
                        db=self.settings.cache.redis_db,
                    ),
                    collection=self.settings.cache.collection_name,
                )
            else:
                logger.warning("Local cache not implemented, using no cache")

        self.pipeline = IngestionPipeline(
            transformations=transformations,
            cache=cache,
        )

    def build_index(self, documents: List[Document]) -> VectorStoreIndex:
        """
        构建向量索引

        Args:
            documents: Document 对象列表

        Returns:
            VectorStoreIndex 对象

        注意：
        - 自动使用缓存，避免重复计算 embedding
        - 索引持久化到 Chroma
        """
        logger.info(f"Starting index build for {len(documents)} documents...")

        # 运行 Pipeline（自动批处理和缓存）
        nodes = self.pipeline.run(documents=documents, show_progress=True)
        logger.info(f"Generated {len(nodes)} nodes from {len(documents)} documents")

        # 创建 Chroma collection
        collection = self.chroma_client.get_or_create_collection(
            name=self.settings.vector_store.collection_name
        )
        vector_store = ChromaVectorStore(chroma_collection=collection)

        # 构建索引
        index = VectorStoreIndex(
            nodes=nodes,
            vector_store=vector_store,
            show_progress=True,
        )

        logger.info("Index build completed and persisted to Chroma")
        return index

    def load_index(self) -> Optional[VectorStoreIndex]:
        """从持久化存储加载索引"""
        try:
            collection = self.chroma_client.get_collection(
                name=self.settings.vector_store.collection_name
            )
            vector_store = ChromaVectorStore(chroma_collection=collection)
            index = VectorStoreIndex.from_vector_store(vector_store)
            logger.info("Index loaded from Chroma successfully")
            return index
        except Exception as e:
            logger.error(f"Failed to load index: {e}")
            return None

    def get_index_stats(self) -> Dict[str, Any]:
        """获取索引统计信息"""
        try:
            collection = self.chroma_client.get_collection(
                name=self.settings.vector_store.collection_name
            )
            return {
                "collection_name": self.settings.vector_store.collection_name,
                "document_count": collection.count(),
                "persist_directory": self.settings.vector_store.persist_directory,
            }
        except:
            return {"error": "Index not found"}
```

**约束规则**:
1. **必须**使用 `IngestionPipeline` 而非手动循环
2. **禁止**跳过缓存配置（即使测试环境也要启用）
3. **必须**记录详细日志（节点数量、耗时）

### 4.3 `core/query_engine.py`（查询引擎）

**功能**: 封装查询逻辑，支持两阶段检索（向量检索 + Reranker）

**实现规范**:
```python
"""
查询引擎模块
要求：
1. 两阶段检索：向量召回 → Reranker 精排
2. 支持自定义查询参数（top_k, threshold）
3. 返回格式化结果（含来源信息）
"""

from typing import List, Dict, Any, Optional
from llama_index.core import VectorStoreIndex
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.postprocessor.dashscope_rerank import DashScopeRerank
from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels
import logging

from config.settings import SETTINGS, DASHSCOPE_API_KEY

logger = logging.getLogger(__name__)

class LargeRAGQueryEngine:
    """查询引擎封装"""

    def __init__(self, index: VectorStoreIndex):
        self.index = index
        self.settings = SETTINGS
        self.api_key = DASHSCOPE_API_KEY

        # 初始化 LLM
        self.llm = DashScope(
            model_name=DashScopeGenerationModels.QWEN_MAX,
            api_key=self.api_key,
            temperature=self.settings.llm.temperature,
            max_tokens=self.settings.llm.max_tokens,
        )

        # 初始化 Reranker
        self.reranker = None
        if self.settings.reranker.enabled:
            self.reranker = DashScopeRerank(
                model=self.settings.reranker.model,
                api_key=self.api_key,
                top_n=self.settings.retrieval.rerank_top_n,
            )

        # 构建查询引擎
        self._build_query_engine()

    def _build_query_engine(self):
        """构建查询引擎（含 Reranker）"""
        # Retriever
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=self.settings.retrieval.similarity_top_k,
        )

        # Query Engine
        postprocessors = [self.reranker] if self.reranker else []
        self.query_engine = RetrieverQueryEngine.from_args(
            retriever=retriever,
            node_postprocessors=postprocessors,
            llm=self.llm,
        )

    def query(self, query_text: str, **kwargs) -> str:
        """
        执行查询

        Args:
            query_text: 查询文本
            **kwargs: 自定义参数（如 top_k）

        Returns:
            LLM 生成的回答
        """
        logger.info(f"Querying: {query_text}")
        response = self.query_engine.query(query_text)
        return str(response)

    def get_similar_documents(
        self,
        query_text: str,
        top_k: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        获取相似文档（不使用 LLM 生成）

        Args:
            query_text: 查询文本
            top_k: 返回数量（默认使用配置值）

        Returns:
            文档列表，格式：[{"text": ..., "score": ..., "metadata": ...}]
        """
        top_k = top_k or self.settings.retrieval.rerank_top_n
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=self.settings.retrieval.similarity_top_k,
        )

        nodes = retriever.retrieve(query_text)

        # Reranker
        if self.reranker:
            nodes = self.reranker.postprocess_nodes(nodes, query_str=query_text)

        # 格式化结果
        results = []
        for node in nodes[:top_k]:
            results.append({
                "text": node.get_content(),
                "score": node.score,
                "metadata": node.metadata,
            })

        return results
```

**测试要求**:
- 必须测试 Reranker 启用/禁用两种情况
- 必须验证返回结果数量符合 `top_k` 设置

### 4.4 `largerag.py`（主接口类）

**功能**: 对外提供统一 API，隐藏内部实现细节

**实现规范**:
```python
"""
LargeRAG 主接口类
要求：
1. 简洁的 API 设计，供 Reasoning Agent 调用
2. 自动处理初始化和资源管理
3. 提供清晰的错误提示
"""

from typing import List, Dict, Any, Optional
from pathlib import Path
import logging

from core.document_processor import DocumentProcessor
from core.indexer import LargeRAGIndexer
from core.query_engine import LargeRAGQueryEngine
from config.settings import SETTINGS

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class LargeRAG:
    """
    LargeRAG 主接口类

    使用示例：
        # 1. 初始化
        rag = LargeRAG()

        # 2. 索引文档（首次运行）
        rag.index_from_folders("data/literature")

        # 3. 查询（后续运行可直接查询，自动加载索引）
        answer = rag.query("如何设计低温 DES 溶剂？")
        print(answer)
    """

    def __init__(self, config_path: Optional[str] = None):
        """
        初始化 LargeRAG

        Args:
            config_path: 配置文件路径（可选，默认使用 config/settings.yaml）
        """
        logger.info("Initializing LargeRAG...")

        self.doc_processor = DocumentProcessor()
        self.indexer = LargeRAGIndexer()
        self.query_engine = None

        # 尝试加载已有索引
        index = self.indexer.load_index()
        if index:
            self.query_engine = LargeRAGQueryEngine(index)
            logger.info("Loaded existing index")
        else:
            logger.warning("No existing index found. Please run index_from_folders() first.")

    def index_from_folders(self, literature_dir: str) -> bool:
        """
        从文献文件夹结构构建索引

        Args:
            literature_dir: 文献目录路径（如 "data/literature"）

        Returns:
            bool: 是否成功

        注意：
        - 如果索引已存在，将覆盖旧索引
        - 使用缓存，重复运行时只计算新增文档的 embedding
        """
        try:
            logger.info(f"Starting indexing process from {literature_dir}...")

            # 1. 处理文档
            documents = self.doc_processor.process_from_folders(literature_dir)
            stats = self.doc_processor.get_statistics()
            logger.info(f"Document processing stats: {stats}")

            if not documents:
                logger.error("No valid documents to index")
                return False

            # 2. 构建索引
            index = self.indexer.build_index(documents)

            # 3. 初始化查询引擎
            self.query_engine = LargeRAGQueryEngine(index)

            logger.info("Indexing completed successfully")
            return True

        except Exception as e:
            logger.error(f"Indexing failed: {e}", exc_info=True)
            return False

    def query(self, query_text: str, **kwargs) -> str:
        """
        执行查询并生成回答

        Args:
            query_text: 查询文本
            **kwargs: 自定义参数

        Returns:
            LLM 生成的回答

        Raises:
            RuntimeError: 如果索引未初始化
        """
        if self.query_engine is None:
            raise RuntimeError(
                "Index not initialized. Please run index_from_folders() first."
            )

        return self.query_engine.query(query_text, **kwargs)

    def get_similar_docs(
        self,
        query_text: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        获取相似文档（不生成回答）

        Args:
            query_text: 查询文本
            top_k: 返回数量

        Returns:
            文档列表
        """
        if self.query_engine is None:
            raise RuntimeError(
                "Index not initialized. Please run index_from_folders() first."
            )

        return self.query_engine.get_similar_documents(query_text, top_k)

    def get_stats(self) -> Dict[str, Any]:
        """获取系统统计信息"""
        return {
            "index_stats": self.indexer.get_index_stats(),
            "doc_processing_stats": self.doc_processor.get_statistics(),
        }
```

**API 设计约束**:
1. **禁止**在主接口类中暴露内部实现细节（如 `IngestionPipeline`）
2. **必须**提供清晰的错误提示（如索引未初始化）
3. **必须**在方法上添加详细的 docstring

---

## 5. 依赖清单（`requirements.txt`）

```txt
# LlamaIndex 核心
llama-index>=0.10.0
llama-index-core>=0.10.0

# DashScope 集成
llama-index-embeddings-dashscope>=0.2.0
llama-index-llms-dashscope>=0.2.0
llama-index-postprocessor-dashscope-rerank>=0.1.0

# 向量数据库
llama-index-vector-stores-chroma>=0.2.0
chromadb>=0.4.0

# 缓存系统
llama-index-storage-kvstore-redis>=0.2.0
redis>=5.0.0

# 配置和工具
pydantic>=2.0.0
python-dotenv>=1.0.0
PyYAML>=6.0

# 测试
pytest>=7.0.0
pytest-asyncio>=0.21.0
```

**约束规则**:
- **禁止**添加未在方案中提及的依赖
- **必须**指定最低版本号

---

## 6. 实施阶段与交付物

### 阶段 1: 基础设施搭建
**任务**:
1. 创建目录结构（严格按照 3.1 执行）
2. 编写 `config/settings.yaml`（使用模板）
3. 实现 `config/settings.py`（参考 CoreRAG）
4. 创建 `requirements.txt`

**交付物**:
- [ ] 完整的目录结构
- [ ] 可加载的配置文件（无报错）
- [ ] 依赖安装脚本 `install.sh` 或 `install.bat`

**验收标准**:
```bash
python -c "from config.settings import SETTINGS; print(SETTINGS.embedding.model)"
# 输出: text-embedding-v3
```

---

### 阶段 2: 核心模块实现
**任务**:
1. 实现 `core/document_processor.py`
2. 实现 `core/indexer.py`
3. 实现 `core/query_engine.py`
4. 实现 `largerag.py`

**交付物**:
- [ ] 四个模块的完整代码
- [ ] 每个模块的单元测试
- [ ] 代码通过 `black` 和 `flake8` 检查

**验收标准**:
```bash
pytest tests/test_document_processor.py -v  # 全部通过
pytest tests/test_indexer.py -v            # 全部通过
pytest tests/test_query_engine.py -v       # 全部通过
```

---

### 阶段 3: 集成测试
**任务**:
1. ��用现有测试数据（35 篇文献，已在 `data/literature/` 目录）
2. 编写 `tests/test_integration.py`
3. 执行端到端测试
4. 性能验证（索引耗时、查询延迟）

**交付物**:
- [ ] 集成测试代码（使用 `data/literature/` 中的 35 篇文献）
- [ ] 测试报告（Markdown 格式）

**测试用例**（强制执行）:
```python
def test_end_to_end_workflow():
    """端到端工作流测试"""
    # 1. 构建索引（使用 data/literature 中的 35 篇文献）
    rag = LargeRAG()
    assert rag.index_from_folders("data/literature") == True

    # 2. 查询测试
    answer = rag.query("What are deep eutectic solvents?")
    assert len(answer) > 0

    # 3. 相似文档检索
    docs = rag.get_similar_docs("DES properties", top_k=5)
    assert len(docs) == 5
    assert all("text" in doc for doc in docs)
    assert all("metadata" in doc for doc in docs)

    # 4. 统计信息
    stats = rag.get_stats()
    assert stats["index_stats"]["document_count"] > 0
    assert stats["doc_processing_stats"]["processed"] > 0

def test_folder_structure_handling():
    """测试文件夹结构处理"""
    from core.document_processor import DocumentProcessor

    processor = DocumentProcessor()
    documents = processor.process_from_folders("data/literature")

    # 验证文档加载
    assert len(documents) > 0

    # 验证元数据
    for doc in documents[:5]:  # 检查前 5 个文档
        assert "doc_hash" in doc.metadata
        assert "page_idx" in doc.metadata
        assert "source_file" in doc.metadata

    # 验证统计信息
    stats = processor.get_statistics()
    assert stats["processed"] > 0
```

**验收标准**:
- 索引 35 篇文献 < 3 分钟（含 embedding）
- 查询响应时间 < 10 秒（含 LLM 生成）
- 所有测试用例通过
- 文档加载成功率 > 95%

---

### 阶段 4: 文档与示例
**任务**:
1. 编写 `README.md`（使用说明）
2. 提供使用示例脚本 `examples/basic_usage.py`
3. 编写故障排查指南
4. 代码注释补充

**交付物**:
- [ ] `README.md`（含安装、配置、使用三部分）
- [ ] `examples/basic_usage.py`（可直接运行）
- [ ] `docs/troubleshooting.md`（常见问题）

**README 结构**（强制要求）:
```markdown
# LargeRAG 工具

## 1. 功能简介
（简述 LargeRAG 的作用）

## 2. 数据格式说明
（文献文件夹结构说明，content_list_process.json 格式）

## 3. 安装
（依赖安装步骤）

## 4. 配置
（如何设置 .env 和 settings.yaml）

## 5. 快速开始
（基本使用示例：从文件夹索引和查询）

## 6. API 文档
（LargeRAG 类的方法说明）

## 7. 常见问题
（FAQ）
```

**`examples/basic_usage.py` 模板**:
```python
"""
LargeRAG 基本使用示例

演示：
1. 从文献文件夹构建索引
2. 执行查询
3. 获取相似文档
4. 查看统计信息
"""

from largerag import LargeRAG
import logging

logging.basicConfig(level=logging.INFO)

def main():
    # 1. 初始化 LargeRAG
    print("=== 初始化 LargeRAG ===")
    rag = LargeRAG()

    # 2. 索引文献（首次运行）
    print("\n=== 索引文献 ===")
    literature_dir = "data/literature"  # 35 篇测试文献
    success = rag.index_from_folders(literature_dir)

    if not success:
        print("索引构建失败")
        return

    print("索引构建成功！")

    # 3. 查询示例
    print("\n=== 查询示例 1: 生成回答 ===")
    query1 = "What are deep eutectic solvents?"
    answer = rag.query(query1)
    print(f"Q: {query1}")
    print(f"A: {answer[:200]}...\n")

    # 4. 相似文档检索（不生成回答）
    print("\n=== 查询示例 2: 检索相似文档 ===")
    query2 = "DES properties and applications"
    docs = rag.get_similar_docs(query2, top_k=3)

    print(f"Q: {query2}")
    for i, doc in enumerate(docs, 1):
        print(f"\n[文档 {i}] (score: {doc['score']:.3f})")
        print(f"来源: {doc['metadata']['doc_hash']}")
        print(f"页码: {doc['metadata']['page_idx']}")
        print(f"内容: {doc['text'][:150]}...")

    # 5. 统计信息
    print("\n=== 系统统计信息 ===")
    stats = rag.get_stats()
    print(f"索引文档数: {stats['index_stats']['document_count']}")
    print(f"处理成功: {stats['doc_processing_stats']['processed']}")
    print(f"跳过文档: {stats['doc_processing_stats']['skipped']}")

if __name__ == "__main__":
    main()
```

---

## 7. 质量保证要求

### 7.1 代码规范（强制执行）
- **必须**使用 Type Hints（所有函数参数和返回值）
- **必须**编写 Docstring（Google 风格）
- **必须**通过 `black` 格式化（行宽 88）
- **必须**通过 `flake8` 检查（无警告）
- **禁止**使用 `print()` 调试，必须用 `logging`

### 7.2 测试覆盖率要求
- 核心模块测试覆盖率 >= 80%
- 所有公共方法必须有测试用例
- 必须测试边界情况和异常处理

### 7.3 性能要求
| 操作 | 数据规模 | 性能指标 |
|------|---------|---------|
| 索引构建 | 100 篇 | < 5 分钟 |
| 索引构建 | 10,000 篇 | < 8 小时 |
| 查询响应 | 任意 | < 10 秒 |
| 缓存命中 | 重复文档 | embedding 计算时间 < 1 秒 |

### 7.4 错误处理要求
**必须捕获的异常**:
- API Key 缺失或无效
- 网络请求失败（DashScope API）
- Redis 连接失败
- Chroma 数据库损坏
- JSON 数据格式错误

**错误信息要求**:
- 必须包含错误原因
- 必须包含解决建议
- 必须记录到日志文件

---

## 8. 验收标准（最终检查清单）

### 8.1 功能验收
- [ ] 可以从文献文件夹结构构建索引
- [ ] 正确处理 content_list_process.json 和 article.json 两种格式
- [ ] 索引持久化到 Chroma（重启后可加载）
- [ ] 查询返回正确结果（含来源信息）
- [ ] Reranker 正确工作（排序改进）
- [ ] 缓存生效（重复运行时速度提升）

### 8.2 代码质量验收
- [ ] 所有代码通过 `black` 和 `flake8`
- [ ] 所有公共方法有 Docstring
- [ ] 测试覆盖率 >= 80%
- [ ] 无 `TODO` 或 `FIXME` 注释

### 8.3 文档验收
- [ ] README.md 完整且可按步骤操作
- [ ] examples/basic_usage.py 可直接运行
- [ ] 配置文件有详细注释
- [ ] API 文档清晰

### 8.4 性能验收
- [ ] 100 篇文献索引 < 5 分钟
- [ ] 查询响应 < 10 秒
- [ ] 缓存命中时速度提升 > 90%

---

## 9. 风险与注意事项

### 9.1 已知风险
| 风险 | 影响 | 缓解措施 |
|------|------|---------|
| DashScope API 配额限制 | 索引失败 | 实现重试逻辑，记录失败文档 |
| Redis 未安装 | 缓存失败 | 提供 Docker Compose 一键部署 |
| 10K 文献内存占用过大 | OOM | 使用流式处理，分批索引 |
| Chroma 数据库损坏 | 索引丢失 | 提供备份和恢复脚本 |

### 9.2 调试技巧
1. **Embedding 失败**: 检查 `DASHSCOPE_API_KEY` 是否正确
2. **缓存未生效**: 检查 Redis 是否运行（`redis-cli ping`）
3. **查询无结果**: 检查索引是否构建（`rag.get_stats()`）
4. **内存占用高**: 减小 `chunk_size` 或分批处理

---

## 10. 后续扩展方向

### 10.1 近期优化（可选）
- 支持增量索引（新增文献时无需重建）
- 多语言支持（中英文混合检索）
- 混合检索（BM25 + 向量检索）

### 10.2 长期规划
- 与 Reasoning Agent 的接口协议
- 支持本地部署 Qwen 模型
- 分布式索引（支持 10 万+ 文献）

---

## 附录 A: 关键依赖版本兼容性

| 依赖 | 最低版本 | 推荐版本 | 已知问题 |
|------|---------|---------|---------|
| llama-index | 0.10.0 | 0.10.20+ | < 0.10 API 不兼容 |
| chromadb | 0.4.0 | 0.4.24 | < 0.4 持久化有 bug |
| redis | 5.0.0 | 7.2.0 | 无 |

---

## 附录 B: DashScope API 使用限制

| 模型 | QPS 限制 | Token 限制 | 备注 |
|------|---------|-----------|------|
| text-embedding-v2 | 100 | 25 texts/request | 建议 batch_size=25 |
| gte-rerank | 50 | 20 docs/request | - |
| qwen-max | 10 | 8k tokens | 足够生成回答 |

---

**文档版本**: v1.0
**最后更新**: 2025-10-11
**审核人**: [待填写]
**实施人**: [待填写]

---

**重要提醒**:
1. 本方案是**强制规范**，不得随意修改架构和技术选型
2. 每完成一个阶段必须通过验收才能进入下一阶段
3. 遇到技术问题必须先查阅文档和日志，再寻求帮助
4. 所有代码必须提交前运行 `black` 和 `pytest`

**开始实施前必读**:
- [ ] 已阅读并理解全部技术约束
- [ ] 已确认 DashScope API Key 可用
- [ ] 已安装 Redis 并确认可连接
- [ ] 已设置 PROJECT_ROOT 环境变量

祝实施顺利！如有疑问，请参考 CoreRAG 的实现作为参考。
