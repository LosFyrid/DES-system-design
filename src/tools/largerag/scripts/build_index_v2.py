"""
å¢é‡æ„å»ºç´¢å¼•è„šæœ¬ V2 - ä½¿ç”¨æ–‡æ¡£çº§ç¼“å­˜å’Œæ‰¹é‡å†™å…¥
æ”¯æŒçœŸæ­£çš„æ–­ç‚¹ç»­ä¼ 

è¿è¡Œæ–¹å¼ï¼š
    python scripts/build_index_v2.py --literature-dir data/DES_v1_7445 --collection-name des_prod_v1 --batch-size 500
"""

import sys
import argparse
from pathlib import Path
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(project_root))

from src.tools.largerag.core.document_processor import DocumentProcessor
from src.tools.largerag.core.indexer_v2 import LargeRAGIndexerV2

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    parser = argparse.ArgumentParser(description='å¢é‡æ„å»ºå‘é‡ç´¢å¼• (V2 - æ–‡æ¡£çº§ç¼“å­˜)')
    parser.add_argument('--literature-dir', required=True, help='æ–‡çŒ®ç›®å½•')
    parser.add_argument('--collection-name', default='des_prod_v1', help='Collectionåç§°')
    parser.add_argument('--batch-size', type=int, default=500, help='æ¯æ‰¹å†™å…¥çš„nodesæ•°é‡ï¼ˆéæ–‡çŒ®æ•°ï¼‰')
    parser.add_argument('--aggregate-small-chunks', action='store_true', help='èšåˆJSON chunks')

    args = parser.parse_args()

    logger.info("="*80)
    logger.info("  å¢é‡æ„å»ºå‘é‡ç´¢å¼• V2")
    logger.info("="*80)
    logger.info(f"\né…ç½®:")
    logger.info(f"  æ–‡çŒ®ç›®å½•: {args.literature_dir}")
    logger.info(f"  Collection: {args.collection_name}")
    logger.info(f"  æ‰¹é‡å†™å…¥: æ¯{args.batch_size}ä¸ªnodeså†™ä¸€æ¬¡")
    logger.info(f"  èšåˆchunks: {args.aggregate_small_chunks}")

    # éªŒè¯æ–‡çŒ®ç›®å½•
    lit_path = Path(args.literature_dir)
    if not lit_path.exists():
        logger.error(f"æ–‡çŒ®ç›®å½•ä¸å­˜åœ¨: {args.literature_dir}")
        return False

    # åˆå§‹åŒ–ç»„ä»¶
    logger.info("\nåˆå§‹åŒ–ç»„ä»¶...")
    doc_processor = DocumentProcessor(aggregate_small_chunks=args.aggregate_small_chunks)
    indexer = LargeRAGIndexerV2(collection_name=args.collection_name)

    # åŠ è½½æ‰€æœ‰æ–‡æ¡£
    logger.info("\nåŠ è½½æ–‡çŒ®æ–‡æ¡£...")
    documents = doc_processor.process_from_folders(str(lit_path))
    logger.info(f"åŠ è½½å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£")

    # å¢é‡æ„å»ºç´¢å¼•
    index = indexer.build_index_incremental(
        documents=documents,
        batch_write_size=args.batch_size,
        show_progress=True
    )

    # æœ€ç»ˆç»Ÿè®¡
    stats = indexer.get_index_stats()
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“Š æœ€ç»ˆç´¢å¼•ç»Ÿè®¡:")
    logger.info(f"  Collection: {stats['collection_name']}")
    logger.info(f"  æ€»å‘é‡æ•°: {stats['document_count']:,}")
    logger.info(f"  æ•°æ®åº“ä½ç½®: {stats['persist_directory']}")

    if 'cache_stats' in stats:
        cache_stats = stats['cache_stats']
        logger.info(f"\nğŸ“¦ ç¼“å­˜ç»Ÿè®¡:")
        logger.info(f"  ç¼“å­˜ç›®å½•: {cache_stats['cache_dir']}")
        logger.info(f"  å·²ç¼“å­˜æ–‡æ¡£: {cache_stats['cached_documents']}")
        logger.info(f"  ç¼“å­˜å¤§å°: {cache_stats['total_size_mb']} MB")

    logger.info(f"{'='*80}\n")

    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.warning("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        logger.info("å·²å¤„ç†çš„æ–‡æ¡£å·²ä¿å­˜åˆ°Chromaå’Œç¼“å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œç»§ç»­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
