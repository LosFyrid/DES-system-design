"""
åˆ†æ‰¹æ„å»ºç´¢å¼•è„šæœ¬
æ¯500ç¯‡æ–‡çŒ®ç«‹å³å†™å…¥æ•°æ®åº“ï¼Œé™ä½ä¸­æ–­é£é™©

è¿è¡Œæ–¹å¼ï¼š
    python scripts/build_index_batched.py --literature-dir data/DES_v1_7445 --collection-name des_prod_v1 --batch-size 500
"""

import sys
import time
import argparse
from pathlib import Path
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(project_root))

from src.tools.largerag.core.document_processor import DocumentProcessor
from src.tools.largerag.core.indexer import LargeRAGIndexer
from src.tools.largerag.config.settings import SETTINGS

import chromadb
from llama_index.core import StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_processed_doc_hashes(collection) -> set:
    """ä»ç°æœ‰collectionä¸­æå–å·²å¤„ç†çš„æ–‡çŒ®å“ˆå¸Œ"""
    try:
        # è·å–æ‰€æœ‰documentsçš„metadata
        results = collection.get(include=['metadatas'])
        metadatas = results.get('metadatas', [])

        doc_hashes = set()
        for meta in metadatas:
            if meta and 'doc_hash' in meta:
                doc_hashes.add(meta['doc_hash'])

        return doc_hashes
    except:
        return set()


def main():
    parser = argparse.ArgumentParser(description='åˆ†æ‰¹æ„å»ºå‘é‡ç´¢å¼•')
    parser.add_argument('--literature-dir', required=True, help='æ–‡çŒ®ç›®å½•')
    parser.add_argument('--collection-name', default='des_prod_v1', help='Collectionåç§°')
    parser.add_argument('--batch-size', type=int, default=500, help='æ¯æ‰¹å¤„ç†çš„æ–‡çŒ®æ•°é‡')
    parser.add_argument('--aggregate-small-chunks', action='store_true', help='èšåˆJSON chunks')

    args = parser.parse_args()

    logger.info("="*80)
    logger.info("  åˆ†æ‰¹æ„å»ºå‘é‡ç´¢å¼•")
    logger.info("="*80)
    logger.info(f"\né…ç½®:")
    logger.info(f"  æ–‡çŒ®ç›®å½•: {args.literature_dir}")
    logger.info(f"  Collection: {args.collection_name}")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size} ç¯‡/æ‰¹")
    logger.info(f"  èšåˆchunks: {args.aggregate_small_chunks}")

    # éªŒè¯æ–‡çŒ®ç›®å½•
    lit_path = Path(args.literature_dir)
    if not lit_path.exists():
        logger.error(f"æ–‡çŒ®ç›®å½•ä¸å­˜åœ¨: {args.literature_dir}")
        return False

    all_folders = sorted([f for f in lit_path.iterdir() if f.is_dir()])
    total_papers = len(all_folders)
    logger.info(f"  æ€»æ–‡çŒ®æ•°: {total_papers}")

    # åˆå§‹åŒ–ç»„ä»¶
    logger.info("\nåˆå§‹åŒ–ç»„ä»¶...")
    doc_processor = DocumentProcessor(aggregate_small_chunks=args.aggregate_small_chunks)
    indexer = LargeRAGIndexer(collection_name=args.collection_name)

    # æ£€æŸ¥å·²å¤„ç†çš„æ–‡çŒ®
    chroma_client = chromadb.PersistentClient(path=SETTINGS.vector_store.persist_directory)

    try:
        collection = chroma_client.get_collection(name=args.collection_name)
        existing_count = collection.count()
        processed_hashes = get_processed_doc_hashes(collection)
        logger.info(f"\næ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•:")
        logger.info(f"  ç°æœ‰å‘é‡æ•°: {existing_count:,}")
        logger.info(f"  å·²å¤„ç†æ–‡çŒ®: {len(processed_hashes)} ç¯‡")
    except:
        processed_hashes = set()
        logger.info(f"\næœªæ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œå°†ä»å¤´æ„å»º")

    # ç­›é€‰æœªå¤„ç†çš„æ–‡çŒ®
    remaining_folders = [f for f in all_folders if f.name not in processed_hashes]
    logger.info(f"\nå¾…å¤„ç†æ–‡çŒ®: {len(remaining_folders)} ç¯‡")

    if not remaining_folders:
        logger.info("âœ“ æ‰€æœ‰æ–‡çŒ®å·²å¤„ç†å®Œæˆï¼")
        return True

    # åˆ†æ‰¹å¤„ç†
    total_batches = (len(remaining_folders) + args.batch_size - 1) // args.batch_size
    logger.info(f"å°†åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†\n")

    start_time = time.time()
    total_new_nodes = 0

    for batch_idx in range(total_batches):
        batch_start = batch_idx * args.batch_size
        batch_end = min((batch_idx + 1) * args.batch_size, len(remaining_folders))
        batch_folders = remaining_folders[batch_start:batch_end]

        logger.info("="*80)
        logger.info(f"  æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}")
        logger.info(f"  å¤„ç†æ–‡çŒ® {batch_start + 1}-{batch_end} / {len(remaining_folders)}")
        logger.info("="*80)

        batch_start_time = time.time()

        # å¤„ç†æœ¬æ‰¹æ–‡çŒ®
        batch_documents = []
        for folder in batch_folders:
            content_file = folder / "content_list_process.json"
            article_file = folder / "article.json"

            try:
                if content_file.exists():
                    docs = doc_processor._load_from_content_list(content_file, folder.name)
                    batch_documents.extend(docs)
                elif article_file.exists():
                    docs = doc_processor._load_from_article(article_file, folder.name)
                    batch_documents.extend(docs)
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡çŒ® {folder.name} å¤±è´¥: {e}")
                continue

        if not batch_documents:
            logger.warning(f"æ‰¹æ¬¡ {batch_idx + 1} æ— æœ‰æ•ˆæ–‡æ¡£ï¼Œè·³è¿‡")
            continue

        logger.info(f"\nå¤„ç† {len(batch_documents)} ä¸ªæ–‡æ¡£...")

        # è¿è¡Œpipelineï¼ˆparsing + embeddingï¼‰
        try:
            nodes = indexer.pipeline.run(documents=batch_documents, show_progress=True)
            logger.info(f"ç”Ÿæˆ {len(nodes)} ä¸ªnodes")
        except Exception as e:
            logger.error(f"Pipelineå¤„ç†å¤±è´¥: {e}")
            logger.info("å·²å¤„ç†çš„æ‰¹æ¬¡å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œç»§ç»­")
            return False

        # å†™å…¥Chroma
        logger.info(f"å†™å…¥Chromaæ•°æ®åº“...")
        try:
            # è·å–æˆ–åˆ›å»ºcollection
            collection = chroma_client.get_or_create_collection(
                name=args.collection_name,
                metadata={"hnsw:space": SETTINGS.vector_store.distance_metric}
            )
            vector_store = ChromaVectorStore(chroma_collection=collection)
            storage_context = StorageContext.from_defaults(vector_store=vector_store)

            # æ‰¹é‡æ·»åŠ nodes
            from llama_index.core import VectorStoreIndex
            index = VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                embed_model=indexer.embed_model,
                show_progress=False,
            )

            total_new_nodes += len(nodes)

        except Exception as e:
            logger.error(f"å†™å…¥Chromaå¤±è´¥: {e}")
            logger.info("å·²å¤„ç†çš„æ‰¹æ¬¡å·²ä¿å­˜ï¼Œå¯ä»¥é‡æ–°è¿è¡Œç»§ç»­")
            return False

        # æ‰¹æ¬¡å®Œæˆç»Ÿè®¡
        batch_time = time.time() - batch_start_time
        elapsed = time.time() - start_time
        avg_time_per_batch = elapsed / (batch_idx + 1)
        remaining_batches = total_batches - (batch_idx + 1)
        eta = avg_time_per_batch * remaining_batches

        logger.info(f"\nâœ“ æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆ")
        logger.info(f"  æ‰¹æ¬¡è€—æ—¶: {batch_time/60:.1f} åˆ†é’Ÿ")
        logger.info(f"  å·²ç”¨æ—¶é—´: {elapsed/60:.1f} åˆ†é’Ÿ")
        logger.info(f"  é¢„è®¡å‰©ä½™: {eta/60:.1f} åˆ†é’Ÿ")
        logger.info(f"  ç´¯è®¡æ–°å¢: {total_new_nodes:,} nodes")

        # è·å–å½“å‰æ€»æ•°
        current_total = collection.count()
        logger.info(f"  æ•°æ®åº“æ€»è®¡: {current_total:,} vectors\n")

    # å®Œæˆç»Ÿè®¡
    total_time = time.time() - start_time
    logger.info("="*80)
    logger.info("  âœ… å…¨éƒ¨å®Œæˆï¼")
    logger.info("="*80)
    logger.info(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    logger.info(f"  Collection: {args.collection_name}")
    logger.info(f"  æ–°å¤„ç†æ–‡çŒ®: {len(remaining_folders)} ç¯‡")
    logger.info(f"  æ–°å¢nodes: {total_new_nodes:,}")
    logger.info(f"  æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ ({total_time/3600:.2f} å°æ—¶)")

    # æœ€ç»ˆéªŒè¯
    collection = chroma_client.get_collection(name=args.collection_name)
    final_count = collection.count()
    logger.info(f"  æ•°æ®åº“æ€»å‘é‡: {final_count:,}")
    logger.info(f"\næ•°æ®åº“ä½ç½®: {SETTINGS.vector_store.persist_directory}")
    logger.info("="*80 + "\n")

    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.warning("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        logger.info("å·²å¤„ç†çš„æ‰¹æ¬¡å·²ä¿å­˜åˆ°Chromaï¼Œå¯ä»¥é‡æ–°è¿è¡Œç»§ç»­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
