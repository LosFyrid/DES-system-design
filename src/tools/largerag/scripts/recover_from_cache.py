"""
ä»æ˜¨æ™šçš„ç¼“å­˜æ¢å¤ç´¢å¼•å¹¶å¢é‡å¤„ç†å‰©ä½™æ–‡çŒ®

åŠŸèƒ½ï¼š
1. åŠ è½½ç¼“å­˜æ–‡ä»¶ä¸­çš„29.3ä¸‡ä¸ªnodesï¼ˆå·²æœ‰embeddingsï¼‰
2. ååºåˆ—åŒ–å¹¶å†™å…¥Chromaæ•°æ®åº“
3. è¯†åˆ«å·²å¤„ç†çš„æ–‡çŒ®
4. å¢é‡å¤„ç†å‰©ä½™4510ç¯‡æ–‡çŒ®
5. å…¨éƒ¨å†™å…¥åŒä¸€ä¸ªcollection

è¿è¡Œæ–¹å¼ï¼š
    python scripts/recover_from_cache.py
"""

import sys
import pickle
import time
from pathlib import Path
from typing import List, Set
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parents[4]
sys.path.insert(0, str(project_root))

from llama_index.core.schema import BaseNode
from llama_index.core import VectorStoreIndex, Document, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

from src.tools.largerag.config.settings import SETTINGS
from src.tools.largerag.core.document_processor import DocumentProcessor
from src.tools.largerag.core.indexer import LargeRAGIndexer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_cached_nodes(cache_file: Path) -> List[BaseNode]:
    """
    ä»ç¼“å­˜æ–‡ä»¶åŠ è½½nodeså¹¶ååºåˆ—åŒ–

    Args:
        cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„

    Returns:
        ååºåˆ—åŒ–åçš„BaseNodeåˆ—è¡¨
    """
    logger.info(f"åŠ è½½ç¼“å­˜æ–‡ä»¶: {cache_file}")

    with open(cache_file, 'rb') as f:
        cache_data = pickle.load(f)

    # ç¼“å­˜æ ¼å¼: {'nodes': [serialized_node_dict, ...]}
    nodes_data = cache_data.get('nodes', [])
    logger.info(f"ç¼“å­˜ä¸­åŒ…å« {len(nodes_data)} ä¸ªåºåˆ—åŒ–çš„nodes")

    # ååºåˆ—åŒ–nodes
    nodes = []
    for i, node_dict in enumerate(nodes_data):
        try:
            # LlamaIndexä½¿ç”¨ __type__ å’Œ __data__ è¿›è¡Œåºåˆ—åŒ–
            node = BaseNode.from_dict(node_dict)
            nodes.append(node)

            if (i + 1) % 10000 == 0:
                logger.info(f"  å·²ååºåˆ—åŒ– {i + 1}/{len(nodes_data)} ä¸ªnodes...")
        except Exception as e:
            logger.error(f"ååºåˆ—åŒ–node {i} å¤±è´¥: {e}")
            continue

    logger.info(f"âœ“ æˆåŠŸååºåˆ—åŒ– {len(nodes)} ä¸ªnodes")
    return nodes


def extract_processed_doc_hashes(nodes: List[BaseNode]) -> Set[str]:
    """
    ä»nodesçš„metadataä¸­æå–å·²å¤„ç†çš„æ–‡çŒ®å“ˆå¸Œ

    Args:
        nodes: BaseNodeåˆ—è¡¨

    Returns:
        å·²å¤„ç†çš„æ–‡çŒ®å“ˆå¸Œé›†åˆ
    """
    doc_hashes = set()
    for node in nodes:
        metadata = node.metadata
        if 'doc_hash' in metadata:
            doc_hashes.add(metadata['doc_hash'])

    logger.info(f"ä»ç¼“å­˜ä¸­è¯†åˆ«å‡º {len(doc_hashes)} ç¯‡å·²å¤„ç†çš„æ–‡çŒ®")
    return doc_hashes


def write_nodes_to_chroma(
    nodes: List[BaseNode],
    collection_name: str,
    chroma_client: chromadb.PersistentClient,
    indexer: LargeRAGIndexer
) -> VectorStoreIndex:
    """
    å°†nodeså†™å…¥Chromaæ•°æ®åº“

    Args:
        nodes: è¦å†™å…¥çš„nodes
        collection_name: collectionåç§°
        chroma_client: Chromaå®¢æˆ·ç«¯
        indexer: LargeRAGIndexerå®ä¾‹ï¼ˆç”¨äºè·å–embed_modelï¼‰

    Returns:
        VectorStoreIndexå¯¹è±¡
    """
    logger.info(f"åˆ›å»ºChroma collection: {collection_name}")

    # åˆ›å»ºæˆ–è·å–collection
    collection = chroma_client.get_or_create_collection(
        name=collection_name,
        metadata={"hnsw:space": SETTINGS.vector_store.distance_metric}
    )

    vector_store = ChromaVectorStore(chroma_collection=collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    logger.info(f"å†™å…¥ {len(nodes)} ä¸ªnodesåˆ°Chroma...")

    # åˆ›å»ºç´¢å¼•ï¼ˆnodeså·²åŒ…å«embeddingsï¼‰
    index = VectorStoreIndex(
        nodes=nodes,
        storage_context=storage_context,
        embed_model=indexer.embed_model,
        show_progress=True,
    )

    logger.info("âœ“ NodesæˆåŠŸå†™å…¥Chroma")
    return index


def process_remaining_literature(
    literature_dir: str,
    processed_doc_hashes: Set[str],
    indexer: LargeRAGIndexer,
    collection_name: str,
    chroma_client: chromadb.PersistentClient
) -> int:
    """
    å¢é‡å¤„ç†å‰©ä½™çš„æ–‡çŒ®

    Args:
        literature_dir: æ–‡çŒ®ç›®å½•
        processed_doc_hashes: å·²å¤„ç†çš„æ–‡çŒ®å“ˆå¸Œé›†åˆ
        indexer: LargeRAGIndexerå®ä¾‹
        collection_name: collectionåç§°
        chroma_client: Chromaå®¢æˆ·ç«¯

    Returns:
        æ–°å¤„ç†çš„æ–‡çŒ®æ•°é‡
    """
    logger.info("="*80)
    logger.info("å¼€å§‹å¢é‡å¤„ç†å‰©ä½™æ–‡çŒ®")
    logger.info("="*80)

    lit_path = Path(literature_dir)
    all_folders = sorted([f for f in lit_path.iterdir() if f.is_dir()])

    # ç­›é€‰å‡ºæœªå¤„ç†çš„æ–‡çŒ®
    remaining_folders = [f for f in all_folders if f.name not in processed_doc_hashes]

    logger.info(f"æ€»æ–‡çŒ®æ•°: {len(all_folders)}")
    logger.info(f"å·²å¤„ç†: {len(processed_doc_hashes)}")
    logger.info(f"å¾…å¤„ç†: {len(remaining_folders)}")

    if not remaining_folders:
        logger.info("âœ“ æ‰€æœ‰æ–‡çŒ®å·²å¤„ç†å®Œæˆï¼")
        return 0

    # å¤„ç†å‰©ä½™æ–‡çŒ®
    doc_processor = DocumentProcessor(aggregate_small_chunks=True)

    logger.info("\nå¼€å§‹å¤„ç†å‰©ä½™æ–‡çŒ®...")
    logger.info(f"é¢„ä¼°æ—¶é—´: æ ¹æ®APIé™é¢ï¼Œå¯èƒ½éœ€è¦æ•°å°æ—¶\n")

    start_time = time.time()

    # é€ä¸ªå¤„ç†æ–‡çŒ®å¹¶è¿½åŠ åˆ°Chroma
    collection = chroma_client.get_collection(name=collection_name)
    vector_store = ChromaVectorStore(chroma_collection=collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…å†…å­˜å ç”¨è¿‡å¤§ï¼‰
    batch_size = 100
    total_new_nodes = 0

    for i in range(0, len(remaining_folders), batch_size):
        batch_folders = remaining_folders[i:i+batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(remaining_folders) + batch_size - 1) // batch_size

        logger.info(f"\n{'='*60}")
        logger.info(f"æ‰¹æ¬¡ {batch_num}/{total_batches}")
        logger.info(f"å¤„ç†æ–‡çŒ® {i+1}-{min(i+batch_size, len(remaining_folders))}/{len(remaining_folders)}")
        logger.info(f"{'='*60}")

        # å¤„ç†æœ¬æ‰¹æ–‡çŒ®
        batch_documents = []
        for folder in batch_folders:
            content_file = folder / "content_list_process.json"
            article_file = folder / "article.json"

            if content_file.exists():
                docs = doc_processor._load_from_content_list(content_file, folder.name)
                batch_documents.extend(docs)
            elif article_file.exists():
                docs = doc_processor._load_from_article(article_file, folder.name)
                batch_documents.extend(docs)

        if not batch_documents:
            logger.warning(f"æ‰¹æ¬¡ {batch_num} æ— æœ‰æ•ˆæ–‡æ¡£ï¼Œè·³è¿‡")
            continue

        # æ„å»ºç´¢å¼•ï¼ˆä¼šè‡ªåŠ¨è®¡ç®—embeddingå¹¶è¿½åŠ åˆ°Chromaï¼‰
        logger.info(f"å¤„ç† {len(batch_documents)} ä¸ªæ–‡æ¡£...")
        batch_nodes = indexer.pipeline.run(documents=batch_documents, show_progress=True)

        # è¿½åŠ åˆ°ç´¢å¼•
        logger.info(f"è¿½åŠ  {len(batch_nodes)} ä¸ªnodesåˆ°Chroma...")
        for node in batch_nodes:
            vector_store.add([node])

        total_new_nodes += len(batch_nodes)

        elapsed = time.time() - start_time
        avg_time_per_batch = elapsed / batch_num
        remaining_batches = total_batches - batch_num
        eta = avg_time_per_batch * remaining_batches

        logger.info(f"âœ“ æ‰¹æ¬¡ {batch_num} å®Œæˆ")
        logger.info(f"  å·²ç”¨æ—¶é—´: {elapsed/60:.1f} åˆ†é’Ÿ")
        logger.info(f"  é¢„è®¡å‰©ä½™: {eta/60:.1f} åˆ†é’Ÿ")
        logger.info(f"  ç´¯è®¡æ–°å¢: {total_new_nodes} nodes")

    total_time = time.time() - start_time
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ“ å¢é‡å¤„ç†å®Œæˆï¼")
    logger.info(f"  æ–°å¤„ç†æ–‡çŒ®: {len(remaining_folders)} ç¯‡")
    logger.info(f"  æ–°å¢nodes: {total_new_nodes} ä¸ª")
    logger.info(f"  æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ ({total_time/3600:.2f} å°æ—¶)")
    logger.info(f"{'='*80}")

    return len(remaining_folders)


def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*80)
    logger.info("  ä»ç¼“å­˜æ¢å¤å¹¶å¢é‡æ„å»ºç´¢å¼•")
    logger.info("="*80)

    # é…ç½®å‚æ•°
    CACHE_FILE = Path("src/tools/largerag/data/prod_cache/largerag_embedding_cache/2726c8573978c6cf17f8d7b71bae8a66.pkl")
    LITERATURE_DIR = "src/tools/largerag/data/DES_v1_7445"
    COLLECTION_NAME = "des_prod_v1"

    logger.info(f"\né…ç½®:")
    logger.info(f"  ç¼“å­˜æ–‡ä»¶: {CACHE_FILE}")
    logger.info(f"  æ–‡çŒ®ç›®å½•: {LITERATURE_DIR}")
    logger.info(f"  Collection: {COLLECTION_NAME}")

    # æ­¥éª¤1: åŠ è½½ç¼“å­˜çš„nodes
    logger.info(f"\n{'='*80}")
    logger.info("æ­¥éª¤ 1/4: åŠ è½½ç¼“å­˜nodes")
    logger.info(f"{'='*80}")

    if not CACHE_FILE.exists():
        logger.error(f"ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {CACHE_FILE}")
        return False

    cached_nodes = load_cached_nodes(CACHE_FILE)

    if not cached_nodes:
        logger.error("ç¼“å­˜ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­")
        return False

    # æ­¥éª¤2: æå–å·²å¤„ç†çš„æ–‡çŒ®
    logger.info(f"\n{'='*80}")
    logger.info("æ­¥éª¤ 2/4: è¯†åˆ«å·²å¤„ç†çš„æ–‡çŒ®")
    logger.info(f"{'='*80}")

    processed_doc_hashes = extract_processed_doc_hashes(cached_nodes)

    # æ­¥éª¤3: å†™å…¥Chroma
    logger.info(f"\n{'='*80}")
    logger.info("æ­¥éª¤ 3/4: å†™å…¥ç¼“å­˜nodesåˆ°Chroma")
    logger.info(f"{'='*80}")

    # åˆå§‹åŒ–indexerï¼ˆéœ€è¦embed_modelï¼‰
    indexer = LargeRAGIndexer(collection_name=COLLECTION_NAME)
    chroma_client = chromadb.PersistentClient(
        path=SETTINGS.vector_store.persist_directory
    )

    index = write_nodes_to_chroma(
        nodes=cached_nodes,
        collection_name=COLLECTION_NAME,
        chroma_client=chroma_client,
        indexer=indexer
    )

    logger.info(f"âœ“ å·²å†™å…¥ {len(cached_nodes)} ä¸ªnodes")

    # æ­¥éª¤4: å¢é‡å¤„ç†å‰©ä½™æ–‡çŒ®
    logger.info(f"\n{'='*80}")
    logger.info("æ­¥éª¤ 4/4: å¢é‡å¤„ç†å‰©ä½™æ–‡çŒ®")
    logger.info(f"{'='*80}")

    new_count = process_remaining_literature(
        literature_dir=LITERATURE_DIR,
        processed_doc_hashes=processed_doc_hashes,
        indexer=indexer,
        collection_name=COLLECTION_NAME,
        chroma_client=chroma_client
    )

    # å®Œæˆç»Ÿè®¡
    logger.info(f"\n{'='*80}")
    logger.info("  âœ… å…¨éƒ¨å®Œæˆï¼")
    logger.info(f"{'='*80}")

    # è·å–æœ€ç»ˆç»Ÿè®¡
    collection = chroma_client.get_collection(name=COLLECTION_NAME)
    final_count = collection.count()

    logger.info(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    logger.info(f"  Collection: {COLLECTION_NAME}")
    logger.info(f"  æ€»å‘é‡æ•°: {final_count:,}")
    logger.info(f"  ç¼“å­˜æ¢å¤: {len(cached_nodes):,} nodes")
    logger.info(f"  æ–°å¢å¤„ç†: {new_count} ç¯‡æ–‡çŒ®")
    logger.info(f"\næ•°æ®åº“ä½ç½®:")
    logger.info(f"  {SETTINGS.vector_store.persist_directory}")
    logger.info(f"\n{'='*80}\n")

    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.warning("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        logger.info("è¿›åº¦å·²ä¿å­˜åˆ°Chromaï¼Œå¯ä»¥ä½¿ç”¨LargeRAGæ­£å¸¸åŠ è½½")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
