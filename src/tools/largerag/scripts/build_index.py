"""
LargeRAG å‘é‡ç´¢å¼•æ„å»ºå·¥å…·
====================================

åŠŸèƒ½ï¼š
1. ä»æŒ‡å®šæ–‡çŒ®ç›®å½•æ„å»ºå‘é‡ç´¢å¼•
2. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
3. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆé€šè¿‡ç¼“å­˜ï¼‰
4. è¾“å‡ºè¯¦ç»†æ—¥å¿—åˆ°æ–‡ä»¶

è¿è¡Œæ–¹å¼ï¼š
    # åŸºæœ¬è¿è¡Œï¼ˆå¿…é¡»æŒ‡å®šæ–‡çŒ®ç›®å½•ï¼‰
    python scripts/build_index.py --literature-dir data/literature_production

    # æŒ‡å®š collection åç§°
    python scripts/build_index.py --literature-dir data/literature_production --collection-name des_production_v1

    # å¼ºåˆ¶é‡å»ºç´¢å¼•
    python scripts/build_index.py --literature-dir data/literature_production --rebuild

    # ç¦ç”¨ç¼“å­˜ï¼ˆç¡®ä¿ä½¿ç”¨æœ€æ–°é…ç½®ï¼‰
    python scripts/build_index.py --literature-dir data/literature_production --rebuild --no-cache

    # è¦†ç›–æ–‡æ¡£å¤„ç†é…ç½®
    python scripts/build_index.py --literature-dir data/literature --chunk-size 1024 --chunk-overlap 100

    # æŸ¥çœ‹å¸®åŠ©
    python scripts/build_index.py --help

å¯è¦†ç›–çš„é…ç½®å‚æ•°ï¼š
    æ–‡æ¡£å¤„ç†é…ç½®ï¼š
      --splitter-type TYPE           åˆ†å—ç­–ç•¥ (token/semantic/sentence)
      --chunk-size N                 æ–‡æ¡£åˆ†å—å¤§å°
      --chunk-overlap N              æ–‡æ¡£åˆ†å—é‡å å¤§å°
      --separator STR                åˆ†å—åˆ†éš”ç¬¦
      --aggregate-small-chunks       èšåˆJSONæ–‡ä»¶å†…çš„æ‰€æœ‰ç‰‡æ®µ
      --semantic-breakpoint-threshold F  è¯­ä¹‰æ–­ç‚¹é˜ˆå€¼ (0-1)
      --semantic-buffer-size N       è¯­ä¹‰ç¼“å†²åŒºå¤§å°

    å‘é‡å­˜å‚¨é…ç½®ï¼š
      --collection-name NAME         é›†åˆåç§°
      --distance-metric METRIC       è·ç¦»åº¦é‡ (cosine/l2/ip)

    å…¶ä»–ï¼š
      --rebuild                      å¼ºåˆ¶é‡å»ºç´¢å¼•
      --no-cache                     ç¦ç”¨ç¼“å­˜
"""

import sys
import time
import json
import argparse
import logging
from pathlib import Path
from datetime import datetime
from dataclasses import asdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
# build_index.py â†’ scripts â†’ largerag â†’ tools â†’ src â†’ PROJECT_ROOT
project_root = Path(__file__).resolve().parents[4]  # å¾€ä¸Š4çº§
sys.path.insert(0, str(project_root))

from src.tools.largerag import LargeRAG
from src.tools.largerag.config.settings import SETTINGS


def setup_logging(log_dir: Path) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"build_index_{timestamp}.log"

    # é…ç½®æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.DEBUG)

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)

    # é…ç½®æ ¹logger
    logger = logging.getLogger('build_index')
    logger.setLevel(logging.DEBUG)
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger, log_file


def print_section(title: str, logger: logging.Logger):
    """æ‰“å°åˆ†éš”çº¿"""
    separator = "=" * 80
    logger.info("")
    logger.info(separator)
    logger.info(f"  {title}")
    logger.info(separator)


def print_subsection(title: str, logger: logging.Logger):
    """æ‰“å°å­æ ‡é¢˜"""
    logger.info(f"\n--- {title} ---")


def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='LargeRAG å‘é‡ç´¢å¼•æ„å»ºå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ„å»ºç”Ÿäº§ç¯å¢ƒç´¢å¼•ï¼ˆ7000ç¯‡æ–‡çŒ®ï¼‰
  python build_index.py --literature-dir data/literature_production --collection-name des_production_v1

  # å¿«é€Ÿæµ‹è¯•ï¼ˆ35ç¯‡æ–‡çŒ®ï¼‰
  python build_index.py --literature-dir data/literature --collection-name des_test_v1

  # å¼ºåˆ¶é‡å»ºç´¢å¼•
  python build_index.py --literature-dir data/literature --rebuild

  # ç¦ç”¨ç¼“å­˜ï¼ˆæµ‹è¯•é…ç½®å˜åŒ–æ—¶æ¨èï¼‰
  python build_index.py --literature-dir data/literature --rebuild --no-cache

  # è¦†ç›–æ–‡æ¡£å¤„ç†é…ç½®
  python build_index.py --literature-dir data/literature --chunk-size 1024 --chunk-overlap 100
  python build_index.py --literature-dir data/literature --splitter-type semantic

  # è‡ªå®šä¹‰å‘é‡å­˜å‚¨é…ç½®
  python build_index.py --literature-dir data/literature --collection-name test_v2 --distance-metric l2
        """
    )

    # å¿…éœ€å‚æ•°
    parser.add_argument('--literature-dir', type=str, required=True, metavar='PATH',
                       help='æ–‡çŒ®ç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼‰')

    # åŸºæœ¬é€‰é¡¹
    parser.add_argument('--rebuild', action='store_true',
                       help='å¼ºåˆ¶é‡å»ºç´¢å¼•ï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰')
    parser.add_argument('--no-cache', action='store_true',
                       help='ç¦ç”¨ç¼“å­˜ï¼ˆç¡®ä¿ä½¿ç”¨æœ€æ–°é…ç½®é‡å»ºï¼‰')

    # æ–‡æ¡£å¤„ç†é…ç½®
    doc_group = parser.add_argument_group('æ–‡æ¡£å¤„ç†é…ç½®')
    doc_group.add_argument('--splitter-type', type=str, metavar='TYPE',
                          choices=['token', 'semantic', 'sentence'],
                          help='åˆ†å—ç­–ç•¥: token/semantic/sentenceï¼ˆé»˜è®¤: tokenï¼‰')
    doc_group.add_argument('--chunk-size', type=int, metavar='N',
                          help='æ–‡æ¡£åˆ†å—å¤§å°ï¼ˆé»˜è®¤: 512ï¼‰')
    doc_group.add_argument('--chunk-overlap', type=int, metavar='N',
                          help='æ–‡æ¡£åˆ†å—é‡å å¤§å°ï¼ˆé»˜è®¤: 50ï¼‰')
    doc_group.add_argument('--separator', type=str, metavar='STR',
                          help='åˆ†å—åˆ†éš”ç¬¦ï¼ˆé»˜è®¤: \\n\\nï¼‰')
    doc_group.add_argument('--semantic-breakpoint-threshold', type=float, metavar='FLOAT',
                          help='è¯­ä¹‰æ–­ç‚¹é˜ˆå€¼ 0-1ï¼ˆé»˜è®¤: 0.5 â†’ 50%%ï¼Œå€¼è¶Šé«˜è¶Šä¿å®ˆï¼Œä»…semanticæ¨¡å¼ï¼‰')
    doc_group.add_argument('--semantic-buffer-size', type=int, metavar='N',
                          help='è¯­ä¹‰ç¼“å†²åŒºå¤§å°ï¼ˆé»˜è®¤: 1ï¼Œä»…semanticæ¨¡å¼ï¼‰')
    doc_group.add_argument('--aggregate-small-chunks', action='store_true',
                          help='èšåˆJSONæ–‡ä»¶å†…çš„æ‰€æœ‰ç‰‡æ®µä¸ºä¸€ä¸ªDocumentï¼ˆé»˜è®¤: falseï¼‰')

    # å‘é‡å­˜å‚¨é…ç½®
    vector_group = parser.add_argument_group('å‘é‡å­˜å‚¨é…ç½®')
    vector_group.add_argument('--collection-name', type=str, metavar='NAME',
                             help='é›†åˆåç§°ï¼ˆé»˜è®¤: des_literature_v1ï¼‰')
    vector_group.add_argument('--distance-metric', type=str, metavar='METRIC',
                             choices=['cosine', 'l2', 'ip'],
                             help='è·ç¦»åº¦é‡: cosine/l2/ipï¼ˆé»˜è®¤: cosineï¼‰')

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    log_dir = Path(__file__).parent / "logs"
    logger, log_file = setup_logging(log_dir)

    print_section("LargeRAG å‘é‡ç´¢å¼•æ„å»ºå·¥å…·", logger)
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")

    # ============================================================
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–åˆ° SETTINGS
    # ============================================================
    overrides_applied = []

    # ç¼“å­˜é…ç½®
    if args.no_cache:
        SETTINGS.cache.enabled = False
        overrides_applied.append(f"cache.enabled = False")

    # æ–‡æ¡£å¤„ç†é…ç½®
    if args.splitter_type is not None:
        SETTINGS.document_processing.splitter_type = args.splitter_type
        overrides_applied.append(f"document_processing.splitter_type = {args.splitter_type}")

    if args.chunk_size is not None:
        SETTINGS.document_processing.chunk_size = args.chunk_size
        overrides_applied.append(f"document_processing.chunk_size = {args.chunk_size}")

    if args.chunk_overlap is not None:
        SETTINGS.document_processing.chunk_overlap = args.chunk_overlap
        overrides_applied.append(f"document_processing.chunk_overlap = {args.chunk_overlap}")

    if args.separator is not None:
        SETTINGS.document_processing.separator = args.separator
        overrides_applied.append(f"document_processing.separator = {args.separator}")

    if args.semantic_breakpoint_threshold is not None:
        SETTINGS.document_processing.semantic_breakpoint_threshold = args.semantic_breakpoint_threshold
        overrides_applied.append(f"document_processing.semantic_breakpoint_threshold = {args.semantic_breakpoint_threshold}")

    if args.semantic_buffer_size is not None:
        SETTINGS.document_processing.semantic_buffer_size = args.semantic_buffer_size
        overrides_applied.append(f"document_processing.semantic_buffer_size = {args.semantic_buffer_size}")

    if args.aggregate_small_chunks:
        SETTINGS.document_processing.aggregate_small_chunks = True
        overrides_applied.append(f"document_processing.aggregate_small_chunks = True")

    # å‘é‡å­˜å‚¨é…ç½®
    if args.collection_name is not None:
        SETTINGS.vector_store.collection_name = args.collection_name
        overrides_applied.append(f"vector_store.collection_name = {args.collection_name}")

    if args.distance_metric is not None:
        SETTINGS.vector_store.distance_metric = args.distance_metric
        overrides_applied.append(f"vector_store.distance_metric = {args.distance_metric}")

    # æ˜¾ç¤ºå‚æ•°è¦†ç›–ä¿¡æ¯
    if overrides_applied:
        logger.info("\nâš™ï¸  æ£€æµ‹åˆ°å‘½ä»¤è¡Œå‚æ•°è¦†ç›–:")
        for override in overrides_applied:
            logger.info(f"  âœ“ {override}")
        logger.info("")

    # ============================================================
    # 1. éªŒè¯æ–‡çŒ®ç›®å½•
    # ============================================================
    print_section("æ­¥éª¤ 1: éªŒè¯æ–‡çŒ®ç›®å½•", logger)

    literature_dir = Path(args.literature_dir)

    if not literature_dir.exists():
        logger.error(f"âœ— é”™è¯¯: æ–‡çŒ®ç›®å½•ä¸å­˜åœ¨: {literature_dir}")
        return False

    if not literature_dir.is_dir():
        logger.error(f"âœ— é”™è¯¯: è·¯å¾„ä¸æ˜¯ç›®å½•: {literature_dir}")
        return False

    # ç»Ÿè®¡æ–‡çŒ®æ•°é‡
    literature_folders = [d for d in literature_dir.iterdir() if d.is_dir()]
    num_papers = len(literature_folders)

    logger.info(f"\næ–‡çŒ®ç›®å½•: {literature_dir}")
    logger.info(f"âœ“ æ£€æµ‹åˆ° {num_papers} ä¸ªæ–‡çŒ®æ–‡ä»¶å¤¹")

    if num_papers == 0:
        logger.warning("âš ï¸  è­¦å‘Š: æ–‡çŒ®ç›®å½•ä¸ºç©º")
        return False

    # ============================================================
    # 2. åˆå§‹åŒ– LargeRAG
    # ============================================================
    print_section("æ­¥éª¤ 2: åˆå§‹åŒ– LargeRAG", logger)

    collection_name = SETTINGS.vector_store.collection_name
    logger.info(f"\nCollection åç§°: {collection_name}")
    logger.info("(ç‹¬ç«‹çš„ collectionï¼Œä¸ä¼šå½±å“å…¶ä»–å·²æœ‰ç´¢å¼•)")

    start_time = time.time()
    rag = LargeRAG(collection_name=collection_name)
    init_time = time.time() - start_time

    logger.info(f"\nâœ“ LargeRAG åˆå§‹åŒ–å®Œæˆ (è€—æ—¶: {init_time:.2f}ç§’)")

    # æ˜¾ç¤ºå½“å‰é…ç½®
    logger.info(f"\nå½“å‰é…ç½®å‚æ•°:")
    logger.info(f"  - Embeddingæ¨¡å‹:  {SETTINGS.embedding.model}")
    logger.info(f"  - æ‰¹å¤„ç†å¤§å°:     {SETTINGS.embedding.batch_size}")
    logger.info(f"  - å‘é‡ç»´åº¦:       {SETTINGS.embedding.dimension}")
    logger.info(f"  - åˆ†å—ç­–ç•¥:       {SETTINGS.document_processing.splitter_type}")
    logger.info(f"  - åˆ†å—å¤§å°:       {SETTINGS.document_processing.chunk_size}")
    logger.info(f"  - åˆ†å—é‡å :       {SETTINGS.document_processing.chunk_overlap}")
    logger.info(f"  - ç¼“å­˜å¯ç”¨:       {SETTINGS.cache.enabled}")
    logger.info(f"  - ç¼“å­˜ç±»å‹:       {SETTINGS.cache.type}")

    # ============================================================
    # 3. æ„å»ºç´¢å¼•ï¼ˆæˆ–åŠ è½½å·²æœ‰ç´¢å¼•ï¼‰
    # ============================================================
    print_section("æ­¥éª¤ 3: æ„å»º/åŠ è½½ç´¢å¼•", logger)

    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•
    need_rebuild = args.rebuild

    if not need_rebuild and rag.query_engine is not None:
        # æœ‰ç´¢å¼•ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç©º
        stats_temp = rag.get_stats()
        index_count = stats_temp['index_stats'].get('document_count', 0)
        if index_count == 0:
            logger.warning("\nâš ï¸  æ£€æµ‹åˆ°ç´¢å¼•ä¸ºç©ºï¼ˆå¯èƒ½ä¹‹å‰æ„å»ºå¤±è´¥ï¼‰ï¼Œå°†å¼ºåˆ¶é‡å»º...")
            need_rebuild = True
        else:
            logger.info(f"\nâœ“ æ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼ˆ{index_count:,} ä¸ªèŠ‚ç‚¹ï¼‰ï¼Œè·³è¿‡æ„å»ºæ­¥éª¤")
            logger.info("  æç¤º: ä½¿ç”¨ --rebuild å‚æ•°å¯å¼ºåˆ¶é‡å»ºç´¢å¼•")

    if need_rebuild or rag.query_engine is None:
        if need_rebuild:
            logger.info("\nğŸ”„ å¼ºåˆ¶é‡å»ºç´¢å¼•...")
        else:
            logger.info("\næœªæ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œå¼€å§‹æ„å»º...")

        logger.info(f"æ–‡çŒ®æ•°é‡: {num_papers}")
        logger.info(f"æ–‡çŒ®ç›®å½•: {literature_dir}")

        if num_papers > 1000:
            logger.info(f"\nâš ï¸  æ³¨æ„: æ–‡çŒ®æ•°é‡è¾ƒå¤§ï¼ˆ{num_papers}ç¯‡ï¼‰ï¼Œé¢„è®¡éœ€è¦è¾ƒé•¿æ—¶é—´")
            logger.info("  - é¢„ä¼°æ—¶é—´: æ ¹æ®APIé™é¢è€Œå®šï¼Œå¯èƒ½éœ€è¦æ•°å°æ—¶")
            logger.info("  - æ”¯æŒæ–­ç‚¹ç»­ä¼ : å¦‚æœä¸­æ–­ï¼Œé‡æ–°è¿è¡Œç›¸åŒå‘½ä»¤å³å¯ç»§ç»­")
            logger.info("  - ç¼“å­˜æœºåˆ¶: å·²å¤„ç†çš„æ–‡æ¡£ä¼šç¼“å­˜ï¼Œä¸ä¼šé‡å¤è°ƒç”¨API\n")

        logger.info("å¼€å§‹æ„å»ºç´¢å¼•...\n")

        start_time = time.time()
        success = rag.index_from_folders(str(literature_dir))
        index_time = time.time() - start_time

        if not success:
            logger.error("\nâœ— ç´¢å¼•æ„å»ºå¤±è´¥")
            return False

        logger.info(f"\nâœ“ ç´¢å¼•æ„å»ºæˆåŠŸ (æ€»è€—æ—¶: {index_time:.2f}ç§’ / {index_time/60:.2f}åˆ†é’Ÿ)")

    # ============================================================
    # 4. æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
    # ============================================================
    print_section("æ­¥éª¤ 4: ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯", logger)

    stats = rag.get_stats()
    index_stats = stats['index_stats']
    doc_stats = stats['doc_processing_stats']

    logger.info(f"\nğŸ“Š å‘é‡ç´¢å¼•ç»Ÿè®¡:")
    logger.info(f"  Collection:      {index_stats.get('collection_name', 'N/A')}")
    logger.info(f"  ç´¢å¼•èŠ‚ç‚¹æ•°:      {index_stats.get('document_count', 0):,}")
    logger.info(f"  å­˜å‚¨ä½ç½®:        {index_stats.get('persist_directory', 'N/A')}")

    logger.info(f"\nğŸ“Š æ–‡æ¡£å¤„ç†ç»Ÿè®¡:")
    processed = doc_stats.get('processed', 0)
    skipped = doc_stats.get('skipped', 0)
    total = doc_stats.get('total', 0)

    logger.info(f"  å·²å¤„ç†æ–‡æ¡£æ®µè½: {processed:,}")
    logger.info(f"  è·³è¿‡æ–‡æ¡£:        {skipped:,}")
    logger.info(f"  æ€»è®¡:            {total:,}")

    if total > 0:
        success_rate = (processed / total) * 100
        logger.info(f"  æˆåŠŸç‡:          {success_rate:.2f}%")

    # ============================================================
    # 5. ä¿å­˜æ„å»ºæŠ¥å‘Š
    # ============================================================
    print_section("æ­¥éª¤ 5: ä¿å­˜æ„å»ºæŠ¥å‘Š", logger)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(__file__).parent / "build_reports"
    output_dir.mkdir(exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = output_dir / f"build_report_{collection_name}_{timestamp}.json"

    # æ•´åˆæ‰€æœ‰ç»“æœ
    build_report = {
        "build_info": {
            "timestamp": timestamp,
            "literature_dir": str(literature_dir),
            "collection_name": collection_name,
            "num_literature": num_papers,
            "rebuild": args.rebuild,
            "cache_enabled": SETTINGS.cache.enabled,
        },
        "config_parameters": asdict(SETTINGS),
        "index_stats": index_stats,
        "doc_processing_stats": doc_stats,
    }

    # ä¿å­˜åˆ° JSON
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(build_report, f, ensure_ascii=False, indent=2)

    logger.info(f"\nâœ“ æ„å»ºæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # ============================================================
    # 6. å®Œæˆ
    # ============================================================
    print_section("æ„å»ºå®Œæˆï¼", logger)

    logger.info("\nâœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
    logger.info(f"\nç´¢å¼•ä¿¡æ¯:")
    logger.info(f"  Collection:  {collection_name}")
    logger.info(f"  èŠ‚ç‚¹æ•°:      {index_stats.get('document_count', 0):,}")
    logger.info(f"  æ–‡çŒ®æ•°:      {num_papers}")

    logger.info(f"\næ•°æ®æ–‡ä»¶ä½ç½®:")
    logger.info(f"  å‘é‡æ•°æ®åº“:  {index_stats.get('persist_directory', 'N/A')}")
    logger.info(f"  æ—¥å¿—æ–‡ä»¶:    {log_file}")
    logger.info(f"  æ„å»ºæŠ¥å‘Š:    {report_file}")

    logger.info("\nä¸‹ä¸€æ­¥æ“ä½œ:")
    logger.info("  1. éªŒè¯ç´¢å¼•:")
    logger.info(f"     python -c \"from largerag import LargeRAG; rag=LargeRAG(collection_name='{collection_name}'); print(rag.get_stats())\"")
    logger.info("\n  2. æµ‹è¯•æŸ¥è¯¢:")
    logger.info("     python examples/2_query_and_retrieve.py")
    logger.info("\n  3. éƒ¨ç½²åˆ°æœåŠ¡å™¨:")
    logger.info(f"     å¤åˆ¶ {index_stats.get('persist_directory', 'N/A')} åˆ°æœåŠ¡å™¨")

    logger.info("\n" + "=" * 80 + "\n")

    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ„å»º")
        print("æç¤º: é‡æ–°è¿è¡Œç›¸åŒå‘½ä»¤å¯ç»§ç»­æ„å»ºï¼ˆç¼“å­˜ä¼šä¿ç•™å·²å¤„ç†çš„æ–‡æ¡£ï¼‰")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æ„å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
